{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![GitHub](https://img.shields.io/badge/Github-hibana2077-blue?style=plastic-square&logo=github)](https://github.com/hibana2077)\n",
    "[![Colab](https://img.shields.io/badge/Colab-Open%20in%20Colab-blue?style=plastic-square&logo=googlecolab)](https://colab.research.google.com/github/hibana2077/hibana2077/blob/master/train/train.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果要訓練這份資料集會需要安裝talib套件，請參考[這裡](https://www.lfd.uci.edu/~gohlke/pythonlibs/#ta-lib)下載對應的版本，並使用pip安裝。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccxt import binance\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from talib import abstract\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 環境整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_dir = os.listdir(path=\"..\")\n",
    "if \"data\" not in ls_dir:\n",
    "    os.mkdir(path=\"../data\")\n",
    "if \"model\" not in ls_dir:\n",
    "    os.mkdir(path=\"../model\")\n",
    "if \"data\" in ls_dir:\n",
    "    ls_dir = os.listdir(path=\"../data\")\n",
    "    #remove all files in data folder\n",
    "    for file in ls_dir:os.remove(path=\"../data/\"+file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下載資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binance BTC/USDT 1h candles from 2020-01-01 to 2021-01-01\n",
    "\n",
    "binance = binance()\n",
    "symbol = 'BTC/USDT'\n",
    "timeframe = '1h'\n",
    "file_name = f\"../data/{symbol.replace('/', '_')}_{timeframe}.csv\"\n",
    "start = binance.parse8601('2020-01-01T00:00:00Z')\n",
    "end = binance.parse8601('2022-01-01T00:00:00Z')\n",
    "cnt_time = start\n",
    "data = []\n",
    "while cnt_time < end:\n",
    "    ohlcv = binance.fetch_ohlcv(symbol, timeframe, cnt_time)\n",
    "    data += ohlcv\n",
    "    cnt_time = ohlcv[-1][0] + 3600000 # 1h in ms    \n",
    "df = pd.DataFrame(data, columns=['time', 'open', 'high', 'low', 'close', 'volume'])\n",
    "df['time'] = pd.to_datetime(df['time'], unit='ms')\n",
    "df.to_csv(file_name, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讀取資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#如果有下载好的數據，可以直接讀取\n",
    "data_file = '../data/btc_usdt_1h.csv' #-> 可以自行更換\n",
    "df = pd.read_csv(data_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 數據處理\n",
    "\n",
    "- 計算RSI\n",
    "- 計算MACD\n",
    "- 計算OBV\n",
    "- 計算CCI\n",
    "- 改成變化百分比 -> 標準化\n",
    "\n",
    "關於技術指標的說明可以參考[這裡](https://www.investopedia.com/terms/t/technicalindicator.asp)，或是google。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RSI'] = abstract.RSI(df, timeperiod=14)\n",
    "df['MACD'] = abstract.MACD(df, fastperiod=12, slowperiod=26, signalperiod=9)['macd'] #只取MACD\n",
    "df['OBV'] = abstract.OBV(df, timeperiod=14)\n",
    "df['CCI'] = abstract.CCI(df, timeperiod=14)\n",
    "df['OPEN_percent'] = df['open'].pct_change()\n",
    "df['CLOSE_percent'] = df['close'].pct_change()\n",
    "df['HIGH_percent'] = df['high'].pct_change()\n",
    "df['LOW_percent'] = df['low'].pct_change()\n",
    "df['VOLUME_percent'] = df['volume']*df['close'].pct_change()\n",
    "#由於RSI MACD OBV CCI 他們已經是標準化的，所以不需要再標準化"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 設定買賣點\n",
    "\n",
    "將買賣點分為下跌、上漲、不動，並將數據轉成one-hot編碼。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['UP'] = df['CLOSE_percent'].apply(lambda x: 1 if x > 0 else 0)\n",
    "df['DOWN'] = df['CLOSE_percent'].apply(lambda x: 1 if x < 0 else 0)\n",
    "df['UP'] = df['UP'].shift(-1) #shift UP DOWN 一個單位，因為我們要預測的是下一個時間點的漲跌\n",
    "df['DOWN'] = df['DOWN'].shift(-1)\n",
    "\n",
    "df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>RSI</th>\n",
       "      <th>MACD</th>\n",
       "      <th>OBV</th>\n",
       "      <th>CCI</th>\n",
       "      <th>OPEN_percent</th>\n",
       "      <th>CLOSE_percent</th>\n",
       "      <th>HIGH_percent</th>\n",
       "      <th>LOW_percent</th>\n",
       "      <th>VOLUME_percent</th>\n",
       "      <th>UP</th>\n",
       "      <th>DOWN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2020-01-02 09:00:00</td>\n",
       "      <td>7153.57</td>\n",
       "      <td>7165.00</td>\n",
       "      <td>7135.36</td>\n",
       "      <td>7162.01</td>\n",
       "      <td>840.001328</td>\n",
       "      <td>46.653946</td>\n",
       "      <td>-18.772873</td>\n",
       "      <td>-1085.293960</td>\n",
       "      <td>-35.190579</td>\n",
       "      <td>0.005989</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>0.001051</td>\n",
       "      <td>0.003692</td>\n",
       "      <td>0.898195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2020-01-02 10:00:00</td>\n",
       "      <td>7162.18</td>\n",
       "      <td>7180.00</td>\n",
       "      <td>7153.33</td>\n",
       "      <td>7161.83</td>\n",
       "      <td>1446.219984</td>\n",
       "      <td>46.612835</td>\n",
       "      <td>-17.585034</td>\n",
       "      <td>-2531.513944</td>\n",
       "      <td>-4.322773</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.002094</td>\n",
       "      <td>0.002518</td>\n",
       "      <td>-0.036347</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2020-01-02 11:00:00</td>\n",
       "      <td>7161.89</td>\n",
       "      <td>7168.67</td>\n",
       "      <td>7139.03</td>\n",
       "      <td>7139.79</td>\n",
       "      <td>761.156570</td>\n",
       "      <td>41.760421</td>\n",
       "      <td>-18.212168</td>\n",
       "      <td>-3292.670514</td>\n",
       "      <td>-32.074422</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.003077</td>\n",
       "      <td>-0.001578</td>\n",
       "      <td>-0.001999</td>\n",
       "      <td>-2.342403</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2020-01-02 12:00:00</td>\n",
       "      <td>7139.73</td>\n",
       "      <td>7163.40</td>\n",
       "      <td>7139.03</td>\n",
       "      <td>7158.29</td>\n",
       "      <td>794.030497</td>\n",
       "      <td>46.769498</td>\n",
       "      <td>-17.020182</td>\n",
       "      <td>-2498.640017</td>\n",
       "      <td>-11.448699</td>\n",
       "      <td>-0.003094</td>\n",
       "      <td>0.002591</td>\n",
       "      <td>-0.000735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.057422</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2020-01-02 13:00:00</td>\n",
       "      <td>7158.86</td>\n",
       "      <td>7163.35</td>\n",
       "      <td>7107.43</td>\n",
       "      <td>7131.15</td>\n",
       "      <td>1566.280693</td>\n",
       "      <td>41.174606</td>\n",
       "      <td>-18.057343</td>\n",
       "      <td>-4064.920710</td>\n",
       "      <td>-69.799501</td>\n",
       "      <td>0.002679</td>\n",
       "      <td>-0.003791</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.004426</td>\n",
       "      <td>-5.938410</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17994</th>\n",
       "      <td>2022-01-21 01:00:00</td>\n",
       "      <td>40894.81</td>\n",
       "      <td>40929.76</td>\n",
       "      <td>39730.00</td>\n",
       "      <td>39762.99</td>\n",
       "      <td>4172.351850</td>\n",
       "      <td>25.308075</td>\n",
       "      <td>-334.514225</td>\n",
       "      <td>697285.662621</td>\n",
       "      <td>-152.190082</td>\n",
       "      <td>0.005258</td>\n",
       "      <td>-0.027676</td>\n",
       "      <td>-0.004142</td>\n",
       "      <td>-0.016682</td>\n",
       "      <td>-115.474572</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17995</th>\n",
       "      <td>2022-01-21 02:00:00</td>\n",
       "      <td>39763.00</td>\n",
       "      <td>40280.00</td>\n",
       "      <td>39267.86</td>\n",
       "      <td>39830.52</td>\n",
       "      <td>4354.458167</td>\n",
       "      <td>26.469820</td>\n",
       "      <td>-445.353256</td>\n",
       "      <td>701640.120788</td>\n",
       "      <td>-137.983834</td>\n",
       "      <td>-0.027676</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>-0.015875</td>\n",
       "      <td>-0.011632</td>\n",
       "      <td>7.395233</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17996</th>\n",
       "      <td>2022-01-21 03:00:00</td>\n",
       "      <td>39835.99</td>\n",
       "      <td>39944.45</td>\n",
       "      <td>38220.00</td>\n",
       "      <td>38465.65</td>\n",
       "      <td>9508.823455</td>\n",
       "      <td>19.775068</td>\n",
       "      <td>-635.996220</td>\n",
       "      <td>692131.297333</td>\n",
       "      <td>-150.716081</td>\n",
       "      <td>0.001836</td>\n",
       "      <td>-0.034267</td>\n",
       "      <td>-0.008330</td>\n",
       "      <td>-0.026685</td>\n",
       "      <td>-325.838273</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17997</th>\n",
       "      <td>2022-01-21 04:00:00</td>\n",
       "      <td>38465.65</td>\n",
       "      <td>39070.00</td>\n",
       "      <td>38437.15</td>\n",
       "      <td>38842.03</td>\n",
       "      <td>3091.420650</td>\n",
       "      <td>25.379860</td>\n",
       "      <td>-748.087930</td>\n",
       "      <td>695222.717983</td>\n",
       "      <td>-126.902770</td>\n",
       "      <td>-0.034400</td>\n",
       "      <td>0.009785</td>\n",
       "      <td>-0.021892</td>\n",
       "      <td>0.005682</td>\n",
       "      <td>30.249038</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17998</th>\n",
       "      <td>2022-01-21 05:00:00</td>\n",
       "      <td>38842.03</td>\n",
       "      <td>38998.00</td>\n",
       "      <td>38700.19</td>\n",
       "      <td>38909.01</td>\n",
       "      <td>2658.659590</td>\n",
       "      <td>26.365761</td>\n",
       "      <td>-822.040718</td>\n",
       "      <td>697881.377573</td>\n",
       "      <td>-106.568770</td>\n",
       "      <td>0.009785</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>-0.001843</td>\n",
       "      <td>0.006843</td>\n",
       "      <td>4.584648</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17966 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      time      open      high       low     close  \\\n",
       "33     2020-01-02 09:00:00   7153.57   7165.00   7135.36   7162.01   \n",
       "34     2020-01-02 10:00:00   7162.18   7180.00   7153.33   7161.83   \n",
       "35     2020-01-02 11:00:00   7161.89   7168.67   7139.03   7139.79   \n",
       "36     2020-01-02 12:00:00   7139.73   7163.40   7139.03   7158.29   \n",
       "37     2020-01-02 13:00:00   7158.86   7163.35   7107.43   7131.15   \n",
       "...                    ...       ...       ...       ...       ...   \n",
       "17994  2022-01-21 01:00:00  40894.81  40929.76  39730.00  39762.99   \n",
       "17995  2022-01-21 02:00:00  39763.00  40280.00  39267.86  39830.52   \n",
       "17996  2022-01-21 03:00:00  39835.99  39944.45  38220.00  38465.65   \n",
       "17997  2022-01-21 04:00:00  38465.65  39070.00  38437.15  38842.03   \n",
       "17998  2022-01-21 05:00:00  38842.03  38998.00  38700.19  38909.01   \n",
       "\n",
       "            volume        RSI        MACD            OBV         CCI  \\\n",
       "33      840.001328  46.653946  -18.772873   -1085.293960  -35.190579   \n",
       "34     1446.219984  46.612835  -17.585034   -2531.513944   -4.322773   \n",
       "35      761.156570  41.760421  -18.212168   -3292.670514  -32.074422   \n",
       "36      794.030497  46.769498  -17.020182   -2498.640017  -11.448699   \n",
       "37     1566.280693  41.174606  -18.057343   -4064.920710  -69.799501   \n",
       "...            ...        ...         ...            ...         ...   \n",
       "17994  4172.351850  25.308075 -334.514225  697285.662621 -152.190082   \n",
       "17995  4354.458167  26.469820 -445.353256  701640.120788 -137.983834   \n",
       "17996  9508.823455  19.775068 -635.996220  692131.297333 -150.716081   \n",
       "17997  3091.420650  25.379860 -748.087930  695222.717983 -126.902770   \n",
       "17998  2658.659590  26.365761 -822.040718  697881.377573 -106.568770   \n",
       "\n",
       "       OPEN_percent  CLOSE_percent  HIGH_percent  LOW_percent  VOLUME_percent  \\\n",
       "33         0.005989       0.001069      0.001051     0.003692        0.898195   \n",
       "34         0.001204      -0.000025      0.002094     0.002518       -0.036347   \n",
       "35        -0.000040      -0.003077     -0.001578    -0.001999       -2.342403   \n",
       "36        -0.003094       0.002591     -0.000735     0.000000        2.057422   \n",
       "37         0.002679      -0.003791     -0.000007    -0.004426       -5.938410   \n",
       "...             ...            ...           ...          ...             ...   \n",
       "17994      0.005258      -0.027676     -0.004142    -0.016682     -115.474572   \n",
       "17995     -0.027676       0.001698     -0.015875    -0.011632        7.395233   \n",
       "17996      0.001836      -0.034267     -0.008330    -0.026685     -325.838273   \n",
       "17997     -0.034400       0.009785     -0.021892     0.005682       30.249038   \n",
       "17998      0.009785       0.001724     -0.001843     0.006843        4.584648   \n",
       "\n",
       "        UP  DOWN  \n",
       "33     0.0   1.0  \n",
       "34     0.0   1.0  \n",
       "35     1.0   0.0  \n",
       "36     0.0   1.0  \n",
       "37     1.0   0.0  \n",
       "...    ...   ...  \n",
       "17994  1.0   0.0  \n",
       "17995  0.0   1.0  \n",
       "17996  1.0   0.0  \n",
       "17997  1.0   0.0  \n",
       "17998  0.0   1.0  \n",
       "\n",
       "[17966 rows x 17 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\李軒豪\\AppData\\Local\\Temp\\ipykernel_34144\\3759880002.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['RSI'] = (df['RSI'] - df['RSI'].min()) / (df['RSI'].max() - df['RSI'].min())\n",
      "C:\\Users\\李軒豪\\AppData\\Local\\Temp\\ipykernel_34144\\3759880002.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['MACD'] = (df['MACD'] - df['MACD'].min()) / (df['MACD'].max() - df['MACD'].min())\n",
      "C:\\Users\\李軒豪\\AppData\\Local\\Temp\\ipykernel_34144\\3759880002.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['OBV'] = (df['OBV'] - df['OBV'].min()) / (df['OBV'].max() - df['OBV'].min())\n",
      "C:\\Users\\李軒豪\\AppData\\Local\\Temp\\ipykernel_34144\\3759880002.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['CCI'] = (df['CCI'] - df['CCI'].min()) / (df['CCI'].max() - df['CCI'].min())\n",
      "C:\\Users\\李軒豪\\AppData\\Local\\Temp\\ipykernel_34144\\3759880002.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['OPEN_percent'] = (df['OPEN_percent'] - df['OPEN_percent'].min()) / (df['OPEN_percent'].max() - df['OPEN_percent'].min())\n",
      "C:\\Users\\李軒豪\\AppData\\Local\\Temp\\ipykernel_34144\\3759880002.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['CLOSE_percent'] = (df['CLOSE_percent'] - df['CLOSE_percent'].min()) / (df['CLOSE_percent'].max() - df['CLOSE_percent'].min())\n",
      "C:\\Users\\李軒豪\\AppData\\Local\\Temp\\ipykernel_34144\\3759880002.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['HIGH_percent'] = (df['HIGH_percent'] - df['HIGH_percent'].min()) / (df['HIGH_percent'].max() - df['HIGH_percent'].min())\n",
      "C:\\Users\\李軒豪\\AppData\\Local\\Temp\\ipykernel_34144\\3759880002.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['LOW_percent'] = (df['LOW_percent'] - df['LOW_percent'].min()) / (df['LOW_percent'].max() - df['LOW_percent'].min())\n",
      "C:\\Users\\李軒豪\\AppData\\Local\\Temp\\ipykernel_34144\\3759880002.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['VOLUME_percent'] = (df['VOLUME_percent'] - df['VOLUME_percent'].min()) / (df['VOLUME_percent'].max() - df['VOLUME_percent'].min())\n"
     ]
    }
   ],
   "source": [
    "#正規化\n",
    "df['RSI'] = (df['RSI'] - df['RSI'].min()) / (df['RSI'].max() - df['RSI'].min())\n",
    "df['MACD'] = (df['MACD'] - df['MACD'].min()) / (df['MACD'].max() - df['MACD'].min())\n",
    "df['OBV'] = (df['OBV'] - df['OBV'].min()) / (df['OBV'].max() - df['OBV'].min())\n",
    "df['CCI'] = (df['CCI'] - df['CCI'].min()) / (df['CCI'].max() - df['CCI'].min())\n",
    "df['OPEN_percent'] = (df['OPEN_percent'] - df['OPEN_percent'].min()) / (df['OPEN_percent'].max() - df['OPEN_percent'].min())\n",
    "df['CLOSE_percent'] = (df['CLOSE_percent'] - df['CLOSE_percent'].min()) / (df['CLOSE_percent'].max() - df['CLOSE_percent'].min())\n",
    "df['HIGH_percent'] = (df['HIGH_percent'] - df['HIGH_percent'].min()) / (df['HIGH_percent'].max() - df['HIGH_percent'].min())\n",
    "df['LOW_percent'] = (df['LOW_percent'] - df['LOW_percent'].min()) / (df['LOW_percent'].max() - df['LOW_percent'].min())\n",
    "df['VOLUME_percent'] = (df['VOLUME_percent'] - df['VOLUME_percent'].min()) / (df['VOLUME_percent'].max() - df['VOLUME_percent'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>RSI</th>\n",
       "      <th>MACD</th>\n",
       "      <th>OBV</th>\n",
       "      <th>CCI</th>\n",
       "      <th>OPEN_percent</th>\n",
       "      <th>CLOSE_percent</th>\n",
       "      <th>HIGH_percent</th>\n",
       "      <th>LOW_percent</th>\n",
       "      <th>VOLUME_percent</th>\n",
       "      <th>UP</th>\n",
       "      <th>DOWN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2020-01-02 09:00:00</td>\n",
       "      <td>7153.57</td>\n",
       "      <td>7165.00</td>\n",
       "      <td>7135.36</td>\n",
       "      <td>7162.01</td>\n",
       "      <td>840.001328</td>\n",
       "      <td>0.439139</td>\n",
       "      <td>0.551337</td>\n",
       "      <td>0.126083</td>\n",
       "      <td>0.462566</td>\n",
       "      <td>0.528817</td>\n",
       "      <td>0.514627</td>\n",
       "      <td>0.520187</td>\n",
       "      <td>0.480495</td>\n",
       "      <td>0.489460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2020-01-02 10:00:00</td>\n",
       "      <td>7162.18</td>\n",
       "      <td>7180.00</td>\n",
       "      <td>7153.33</td>\n",
       "      <td>7161.83</td>\n",
       "      <td>1446.219984</td>\n",
       "      <td>0.438670</td>\n",
       "      <td>0.551617</td>\n",
       "      <td>0.124966</td>\n",
       "      <td>0.495765</td>\n",
       "      <td>0.515369</td>\n",
       "      <td>0.511553</td>\n",
       "      <td>0.524332</td>\n",
       "      <td>0.478217</td>\n",
       "      <td>0.489393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2020-01-02 11:00:00</td>\n",
       "      <td>7161.89</td>\n",
       "      <td>7168.67</td>\n",
       "      <td>7139.03</td>\n",
       "      <td>7139.79</td>\n",
       "      <td>761.156570</td>\n",
       "      <td>0.383355</td>\n",
       "      <td>0.551469</td>\n",
       "      <td>0.124377</td>\n",
       "      <td>0.465917</td>\n",
       "      <td>0.511873</td>\n",
       "      <td>0.502978</td>\n",
       "      <td>0.509739</td>\n",
       "      <td>0.469449</td>\n",
       "      <td>0.489230</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2020-01-02 12:00:00</td>\n",
       "      <td>7139.73</td>\n",
       "      <td>7163.40</td>\n",
       "      <td>7139.03</td>\n",
       "      <td>7158.29</td>\n",
       "      <td>794.030497</td>\n",
       "      <td>0.440456</td>\n",
       "      <td>0.551750</td>\n",
       "      <td>0.124991</td>\n",
       "      <td>0.488101</td>\n",
       "      <td>0.503292</td>\n",
       "      <td>0.518903</td>\n",
       "      <td>0.513089</td>\n",
       "      <td>0.473329</td>\n",
       "      <td>0.489542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2020-01-02 13:00:00</td>\n",
       "      <td>7158.86</td>\n",
       "      <td>7163.35</td>\n",
       "      <td>7107.43</td>\n",
       "      <td>7131.15</td>\n",
       "      <td>1566.280693</td>\n",
       "      <td>0.376677</td>\n",
       "      <td>0.551505</td>\n",
       "      <td>0.123781</td>\n",
       "      <td>0.425343</td>\n",
       "      <td>0.519516</td>\n",
       "      <td>0.500972</td>\n",
       "      <td>0.515984</td>\n",
       "      <td>0.464738</td>\n",
       "      <td>0.488975</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17994</th>\n",
       "      <td>2022-01-21 01:00:00</td>\n",
       "      <td>40894.81</td>\n",
       "      <td>40929.76</td>\n",
       "      <td>39730.00</td>\n",
       "      <td>39762.99</td>\n",
       "      <td>4172.351850</td>\n",
       "      <td>0.195806</td>\n",
       "      <td>0.477013</td>\n",
       "      <td>0.665788</td>\n",
       "      <td>0.336728</td>\n",
       "      <td>0.526761</td>\n",
       "      <td>0.433872</td>\n",
       "      <td>0.499548</td>\n",
       "      <td>0.440951</td>\n",
       "      <td>0.481210</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17995</th>\n",
       "      <td>2022-01-21 02:00:00</td>\n",
       "      <td>39763.00</td>\n",
       "      <td>40280.00</td>\n",
       "      <td>39267.86</td>\n",
       "      <td>39830.52</td>\n",
       "      <td>4354.458167</td>\n",
       "      <td>0.209049</td>\n",
       "      <td>0.450922</td>\n",
       "      <td>0.669153</td>\n",
       "      <td>0.352008</td>\n",
       "      <td>0.434217</td>\n",
       "      <td>0.516395</td>\n",
       "      <td>0.452915</td>\n",
       "      <td>0.450752</td>\n",
       "      <td>0.489920</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17996</th>\n",
       "      <td>2022-01-21 03:00:00</td>\n",
       "      <td>39835.99</td>\n",
       "      <td>39944.45</td>\n",
       "      <td>38220.00</td>\n",
       "      <td>38465.65</td>\n",
       "      <td>9508.823455</td>\n",
       "      <td>0.132732</td>\n",
       "      <td>0.406045</td>\n",
       "      <td>0.661804</td>\n",
       "      <td>0.338314</td>\n",
       "      <td>0.517145</td>\n",
       "      <td>0.415356</td>\n",
       "      <td>0.482901</td>\n",
       "      <td>0.421536</td>\n",
       "      <td>0.466298</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17997</th>\n",
       "      <td>2022-01-21 04:00:00</td>\n",
       "      <td>38465.65</td>\n",
       "      <td>39070.00</td>\n",
       "      <td>38437.15</td>\n",
       "      <td>38842.03</td>\n",
       "      <td>3091.420650</td>\n",
       "      <td>0.196624</td>\n",
       "      <td>0.379660</td>\n",
       "      <td>0.664193</td>\n",
       "      <td>0.363926</td>\n",
       "      <td>0.415324</td>\n",
       "      <td>0.539112</td>\n",
       "      <td>0.429001</td>\n",
       "      <td>0.484356</td>\n",
       "      <td>0.491540</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17998</th>\n",
       "      <td>2022-01-21 05:00:00</td>\n",
       "      <td>38842.03</td>\n",
       "      <td>38998.00</td>\n",
       "      <td>38700.19</td>\n",
       "      <td>38909.01</td>\n",
       "      <td>2658.659590</td>\n",
       "      <td>0.207863</td>\n",
       "      <td>0.362251</td>\n",
       "      <td>0.666248</td>\n",
       "      <td>0.385796</td>\n",
       "      <td>0.539482</td>\n",
       "      <td>0.516468</td>\n",
       "      <td>0.508687</td>\n",
       "      <td>0.486611</td>\n",
       "      <td>0.489721</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17966 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      time      open      high       low     close  \\\n",
       "33     2020-01-02 09:00:00   7153.57   7165.00   7135.36   7162.01   \n",
       "34     2020-01-02 10:00:00   7162.18   7180.00   7153.33   7161.83   \n",
       "35     2020-01-02 11:00:00   7161.89   7168.67   7139.03   7139.79   \n",
       "36     2020-01-02 12:00:00   7139.73   7163.40   7139.03   7158.29   \n",
       "37     2020-01-02 13:00:00   7158.86   7163.35   7107.43   7131.15   \n",
       "...                    ...       ...       ...       ...       ...   \n",
       "17994  2022-01-21 01:00:00  40894.81  40929.76  39730.00  39762.99   \n",
       "17995  2022-01-21 02:00:00  39763.00  40280.00  39267.86  39830.52   \n",
       "17996  2022-01-21 03:00:00  39835.99  39944.45  38220.00  38465.65   \n",
       "17997  2022-01-21 04:00:00  38465.65  39070.00  38437.15  38842.03   \n",
       "17998  2022-01-21 05:00:00  38842.03  38998.00  38700.19  38909.01   \n",
       "\n",
       "            volume       RSI      MACD       OBV       CCI  OPEN_percent  \\\n",
       "33      840.001328  0.439139  0.551337  0.126083  0.462566      0.528817   \n",
       "34     1446.219984  0.438670  0.551617  0.124966  0.495765      0.515369   \n",
       "35      761.156570  0.383355  0.551469  0.124377  0.465917      0.511873   \n",
       "36      794.030497  0.440456  0.551750  0.124991  0.488101      0.503292   \n",
       "37     1566.280693  0.376677  0.551505  0.123781  0.425343      0.519516   \n",
       "...            ...       ...       ...       ...       ...           ...   \n",
       "17994  4172.351850  0.195806  0.477013  0.665788  0.336728      0.526761   \n",
       "17995  4354.458167  0.209049  0.450922  0.669153  0.352008      0.434217   \n",
       "17996  9508.823455  0.132732  0.406045  0.661804  0.338314      0.517145   \n",
       "17997  3091.420650  0.196624  0.379660  0.664193  0.363926      0.415324   \n",
       "17998  2658.659590  0.207863  0.362251  0.666248  0.385796      0.539482   \n",
       "\n",
       "       CLOSE_percent  HIGH_percent  LOW_percent  VOLUME_percent   UP  DOWN  \n",
       "33          0.514627      0.520187     0.480495        0.489460  0.0   1.0  \n",
       "34          0.511553      0.524332     0.478217        0.489393  0.0   1.0  \n",
       "35          0.502978      0.509739     0.469449        0.489230  1.0   0.0  \n",
       "36          0.518903      0.513089     0.473329        0.489542  0.0   1.0  \n",
       "37          0.500972      0.515984     0.464738        0.488975  1.0   0.0  \n",
       "...              ...           ...          ...             ...  ...   ...  \n",
       "17994       0.433872      0.499548     0.440951        0.481210  1.0   0.0  \n",
       "17995       0.516395      0.452915     0.450752        0.489920  0.0   1.0  \n",
       "17996       0.415356      0.482901     0.421536        0.466298  1.0   0.0  \n",
       "17997       0.539112      0.429001     0.484356        0.491540  1.0   0.0  \n",
       "17998       0.516468      0.508687     0.486611        0.489721  0.0   1.0  \n",
       "\n",
       "[17966 rows x 17 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    9186\n",
       "0.0    8780\n",
       "Name: UP, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['UP'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    9189\n",
       "1.0    8777\n",
       "Name: DOWN, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['DOWN'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看起來數據蠻平衡的"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 儲存資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(file_name, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分割成X、y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X,y = list(),list()\n",
    "ref_bar = 10\n",
    "\n",
    "for i in range(len(df)-ref_bar):\n",
    "    #df.iloc.values 會回傳一個numpy array\n",
    "    X.append(df.iloc[i:i+ref_bar, 6:15].values) # i to i+ref_bar-1\n",
    "    y.append(df.iloc[i+ref_bar-1, 15:].values) # i+ref_bar-1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43913889, 0.55133701, 0.12608327, 0.46256588, 0.52881702,\n",
       "        0.51462743, 0.52018715, 0.48049549, 0.48945974],\n",
       "       [0.43867025, 0.55161662, 0.12496563, 0.49576539, 0.51536909,\n",
       "        0.51155285, 0.52433208, 0.47821686, 0.48939349],\n",
       "       [0.38335495, 0.551469  , 0.1243774 , 0.46591742, 0.5118732 ,\n",
       "        0.5029779 , 0.50973939, 0.46944876, 0.48923002],\n",
       "       [0.44045613, 0.55174959, 0.12499103, 0.48810118, 0.50329239,\n",
       "        0.51890279, 0.51308937, 0.47332878, 0.48954191],\n",
       "       [0.37667693, 0.55150544, 0.1237806 , 0.42534265, 0.51951603,\n",
       "        0.50097207, 0.51598351, 0.46473758, 0.4889751 ],\n",
       "       [0.39051836, 0.55144598, 0.12451964, 0.45081074, 0.50112199,\n",
       "        0.51337261, 0.51254345, 0.47740044, 0.48943827],\n",
       "       [0.3796175 , 0.55136194, 0.12385315, 0.4362722 , 0.51372472,\n",
       "        0.50980846, 0.51317905, 0.47269111, 0.48935657],\n",
       "       [0.24120493, 0.5498691 , 0.12106759, 0.13851629, 0.50993134,\n",
       "        0.48063427, 0.51314925, 0.44606877, 0.48657753],\n",
       "       [0.15833506, 0.54722152, 0.1162887 , 0.14576122, 0.48122829,\n",
       "        0.47884657, 0.46386846, 0.44699099, 0.48428164],\n",
       "       [0.16968099, 0.54531921, 0.11745842, 0.27270689, 0.47920242,\n",
       "        0.51322764, 0.48027824, 0.48039761, 0.48945733]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]#這裡會出現10個array，每個array裡面有9個數字，分別是RSI,MACD,OBV,CCI,OPEN_percent,CLOSE_percent,HIGH_percent,LOW_percent,VOLUME_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0, 0.0], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "X = torch.tensor(X, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y,dtype=np.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        ...,\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([17956, 10, 9]) , y shape: torch.Size([17956, 2])\n"
     ]
    }
   ],
   "source": [
    "print(f\"X shape: {X.shape} , y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4391, 0.5513, 0.1261, 0.4626, 0.5288, 0.5146, 0.5202, 0.4805, 0.4895],\n",
       "        [0.4387, 0.5516, 0.1250, 0.4958, 0.5154, 0.5116, 0.5243, 0.4782, 0.4894],\n",
       "        [0.3834, 0.5515, 0.1244, 0.4659, 0.5119, 0.5030, 0.5097, 0.4694, 0.4892],\n",
       "        [0.4405, 0.5517, 0.1250, 0.4881, 0.5033, 0.5189, 0.5131, 0.4733, 0.4895],\n",
       "        [0.3767, 0.5515, 0.1238, 0.4253, 0.5195, 0.5010, 0.5160, 0.4647, 0.4890],\n",
       "        [0.3905, 0.5514, 0.1245, 0.4508, 0.5011, 0.5134, 0.5125, 0.4774, 0.4894],\n",
       "        [0.3796, 0.5514, 0.1239, 0.4363, 0.5137, 0.5098, 0.5132, 0.4727, 0.4894],\n",
       "        [0.2412, 0.5499, 0.1211, 0.1385, 0.5099, 0.4806, 0.5131, 0.4461, 0.4866],\n",
       "        [0.1583, 0.5472, 0.1163, 0.1458, 0.4812, 0.4788, 0.4639, 0.4470, 0.4843],\n",
       "        [0.1697, 0.5453, 0.1175, 0.2727, 0.4792, 0.5132, 0.4803, 0.4804, 0.4895]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立資料集類別\n",
    "\n",
    "- 要繼承torch.utils.data.Dataset\n",
    "- 要實作`__len__`、`__getitem__`\n",
    "- 後面要用DataLoader取用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用sklearn來分成train val test\n",
    "# train:val:test = 6:3:1\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.75, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([10773, 10, 9]) , y_train shape: torch.Size([10773, 2])\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train shape: {X_train.shape} , y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: torch.Size([5388, 10, 9]) , y_test shape: torch.Size([5388, 2])\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_test shape: {X_test.shape} , y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_val shape: torch.Size([1795, 10, 9]) , y_val shape: torch.Size([1795, 2])\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_val shape: {X_val.shape} , y_val shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainDataset(X_train, y_train)\n",
    "val_dataset = ValDataset(X_val, y_val)\n",
    "test_dataset = TestDataset(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立模型\n",
    "\n",
    "- 要繼承torch.nn.Module\n",
    "- 可能要多建立不同的模型，到時候看結果再調整 -> 先讓數據流得通，再去看成績做調整。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ver1 -> CNN+MLP\n",
    "- ver2 -> CNN+LSTM+MLP\n",
    "- ver3 -> CNN+GRU+MLP\n",
    "- ver4 -> CNN+LSTM+GRU+MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectItem(torch.nn.Module):#這是用來取出多個輸出其中一個的輸出，如果不用sequential的話，就可以不用這個\n",
    "    def __init__(self, item_index):\n",
    "        super(SelectItem, self).__init__()\n",
    "        self._name = 'selectitem'\n",
    "        self.item_index = item_index\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return inputs[self.item_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class crypto_classfier_ver1(nn.Module): #CNN+MLP\n",
    "    def __init__(self):\n",
    "        super(crypto_classfier_ver1, self).__init__()\n",
    "        self.name = \"CCV-1\"\n",
    "        self.spare_layer = nn.Sequential(\n",
    "            torch.nn.Conv1d(10, 4, 3, stride=4, padding=1),\n",
    "            torch.nn.ReLU())\n",
    "        self.net = nn.Sequential(\n",
    "            torch.nn.Linear(4, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 2),\n",
    "            torch.nn.Softmax(dim=0))\n",
    "        self.fusion = nn.Sequential(\n",
    "            torch.nn.Linear(6, 16),\n",
    "            torch.nn.ELU(),\n",
    "            torch.nn.Linear(16, 2),\n",
    "            torch.nn.Softmax(dim=0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.spare_layer(x)\n",
    "        x = x.T\n",
    "        x1 = self.net(x[0])\n",
    "        x2 = self.net(x[1])\n",
    "        x3 = self.net(x[2])\n",
    "        x = torch.cat((x1,x2,x3),0)\n",
    "        x = self.fusion(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class crypto_classfier_ver2(nn.Module): #CNN+LSTM+MLP\n",
    "    def __init__(self):\n",
    "        super(crypto_classfier_ver2, self).__init__()\n",
    "        self.name = \"CCV-2\"\n",
    "        self.net = nn.Sequential(\n",
    "            torch.nn.Conv1d(10, 20, 3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool1d(1, stride=1),\n",
    "            torch.nn.Conv1d(20, 40, 3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool1d(1, stride=1),\n",
    "            torch.nn.Conv1d(40, 1, 3, stride=1, padding=1),\n",
    "            torch.nn.Linear(9,64),\n",
    "            torch.nn.Linear(64,128),\n",
    "            torch.nn.LSTM(128, 64, 25),\n",
    "            SelectItem(0),\n",
    "            SelectItem(0),\n",
    "            torch.nn.Linear(64,2),\n",
    "            torch.nn.Softmax(dim=0))\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class crypto_classfier_ver3(nn.Module):#CNN+GRU+MLP\n",
    "    def __init__(self):\n",
    "        super(crypto_classfier_ver3, self).__init__()\n",
    "        self.name = \"CCV-3\"\n",
    "        self.net = nn.Sequential(\n",
    "            torch.nn.Conv1d(10, 20, 3, stride=1, padding=1),\n",
    "            torch.nn.ELU(),\n",
    "            torch.nn.MaxPool1d(1, stride=1),\n",
    "            torch.nn.Conv1d(20, 40, 3, stride=1, padding=1),\n",
    "            torch.nn.ELU(),\n",
    "            torch.nn.MaxPool1d(1, stride=1),\n",
    "            torch.nn.Conv1d(40, 1, 3, stride=1, padding=1),\n",
    "            torch.nn.Linear(9,64),\n",
    "            torch.nn.Linear(64,128),\n",
    "            torch.nn.GRU(128, 64, 25),\n",
    "            SelectItem(0),\n",
    "            SelectItem(0),\n",
    "            torch.nn.Linear(64,2),\n",
    "            torch.nn.ELU()\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        # print(f\"=>{F.softmax(x, dim=0)}\")\n",
    "        x = F.softmax(x, dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class crypto_classfier_ver4(nn.Module):#CNN+LSTM+GRU+MLP\n",
    "    def __init__(self):\n",
    "        super(crypto_classfier_ver4, self).__init__()\n",
    "        self.name = \"CCV-4\"\n",
    "        self.batch_size = 1\n",
    "        self.spare_layer = torch.nn.Conv1d(10, 2, 3, stride=3, padding=1)\n",
    "        self.gru = nn.Sequential(\n",
    "            torch.nn.Linear(3, 16),\n",
    "            torch.nn.ELU(),\n",
    "            torch.nn.Linear(16, 128),\n",
    "            torch.nn.ELU(),\n",
    "            torch.nn.GRU(128, 64, 25),\n",
    "            SelectItem(0),\n",
    "            torch.nn.Linear(64, 2))\n",
    "        self.lstm = nn.Sequential(\n",
    "            torch.nn.Linear(3, 16),\n",
    "            torch.nn.ELU(),\n",
    "            torch.nn.Linear(16, 128),\n",
    "            torch.nn.ELU(),\n",
    "            torch.nn.LSTM(128, 64, 25),\n",
    "            SelectItem(0),\n",
    "            torch.nn.Linear(64, 2))\n",
    "        self.fusion = nn.Sequential(\n",
    "            SelectItem(0),\n",
    "            SelectItem(0),\n",
    "            torch.nn.Linear(4, 16),\n",
    "            torch.nn.ELU(),\n",
    "            torch.nn.Linear(16, 2),\n",
    "            torch.nn.Softmax(dim=0)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        x = self.spare_layer(x)\n",
    "        x1 , x2 = x[0], x[1]\n",
    "        x1 = x1.view(1,1,3)\n",
    "        x2 = x2.view(1,1,3)\n",
    "        out1 = self.gru(x1)\n",
    "        out2 = self.lstm(x2)\n",
    "        x = torch.cat((out1,out2),2)\n",
    "        x = self.fusion(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 9])\n",
      "tensor([[0.2092, 0.5178, 0.1727, 0.4007, 0.5123, 0.4674, 0.4951, 0.4368, 0.4837],\n",
      "        [0.2568, 0.5178, 0.1775, 0.3880, 0.4674, 0.5270, 0.4810, 0.4542, 0.4918],\n",
      "        [0.2627, 0.5183, 0.1799, 0.4119, 0.5274, 0.5135, 0.5292, 0.5004, 0.4895],\n",
      "        [0.3035, 0.5199, 0.1816, 0.4255, 0.5136, 0.5245, 0.5259, 0.4711, 0.4901],\n",
      "        [0.2876, 0.5210, 0.1802, 0.4313, 0.5251, 0.5021, 0.5110, 0.4795, 0.4890],\n",
      "        [0.3418, 0.5232, 0.1819, 0.4947, 0.5025, 0.5286, 0.5296, 0.4805, 0.4904],\n",
      "        [0.3860, 0.5262, 0.1845, 0.5713, 0.5293, 0.5263, 0.5403, 0.4846, 0.4906],\n",
      "        [0.3913, 0.5290, 0.1865, 0.5621, 0.5264, 0.5134, 0.5109, 0.4744, 0.4895],\n",
      "        [0.3731, 0.5310, 0.1842, 0.5789, 0.5138, 0.5037, 0.5306, 0.4798, 0.4888],\n",
      "        [0.3455, 0.5321, 0.1826, 0.5364, 0.5040, 0.4993, 0.5073, 0.4638, 0.4887]])\n"
     ]
    }
   ],
   "source": [
    "lab_tensor = train_dataset[0][0]\n",
    "print(lab_tensor.shape)\n",
    "print(lab_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5458, 0.4542], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.4760, 0.5240], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.4858, 0.5142], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.4479, 0.5521], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lab_models = [crypto_classfier_ver1(), crypto_classfier_ver2(), crypto_classfier_ver3(), crypto_classfier_ver4()]\n",
    "out_data = list(map(lambda x: x(lab_tensor), lab_models))\n",
    "\n",
    "for t in out_data:\n",
    "    print(t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "\n",
    "1) model\n",
    "    - `__init__`: define [layers](https://pytorch.org/docs/stable/nn.html)\n",
    "    - forward: forward pass -> compute prediction\n",
    "2) loss and optimizer\n",
    "    - lr: learning rate [default=0.001]\n",
    "    - momentum: momentum for optimizer [default=0.9]\n",
    "    - criterion: loss function [in torch.nn] eg.nn.BCELoss()\n",
    "    - optimizer: optimizer [in torch.optim] eg.torch.optim.SGD()\n",
    "        - eg. optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "3) training loop\n",
    "    - forward pass: compute prediction and loss\n",
    "\n",
    "        ```python\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        ```\n",
    "        \n",
    "    - backward pass: loss.backward()\n",
    "    - update weights: optimizer.step()\n",
    "    - zero the gradients: optimizer.zero_grad()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- 寫訓練方法\n",
    "    - using dataloader\n",
    "        - batch and epoch\n",
    "- 寫驗證方法\n",
    "    - using model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 9])\n",
      "torch.Size([1, 2])\n",
      "tensor(1.)\n",
      "torch.Size([2])\n",
      "tensor([1., 0.])\n",
      "tensor([[0., 1.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\李軒豪\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 2])) that is different to the input size (torch.Size([2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "test_dataloader_temp = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "loss_f = nn.MSELoss()\n",
    "test_model = crypto_classfier_ver1()\n",
    "for ind,(x,y) in enumerate(test_dataloader_temp):\n",
    "    if ind == 50:\n",
    "        print(x.shape)\n",
    "        print(y.shape)\n",
    "        x = x[0]\n",
    "        y = y\n",
    "        out= test_model(x)\n",
    "        out = torch.tensor([1.,0.]) if out[0] > out[1] else torch.tensor([0.,1.])\n",
    "        loss_val = loss_f(out, y)\n",
    "        print(loss_val)\n",
    "        print(out.shape)\n",
    "        print(out)\n",
    "        print(y)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  CCV-1\n",
      "loss_function:  MSELoss()\n",
      "optimizer:  <class 'torch.optim.adam.Adam'>\n",
      "lr:  0.0001\n",
      "epochs:  30\n",
      "batch_size:  1\n",
      "device:  cpu\n",
      "Train epoch:  0 i:  0 loss:  0.39143332839012146 out:  tensor([0.6256, 0.3744], grad_fn=<SoftmaxBackward0>) y:  tensor([[0., 1.]])\n",
      "Train epoch:  0 i:  100 loss:  0.15367677807807922 out:  tensor([0.6080, 0.3920], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  200 loss:  0.3598342835903168 out:  tensor([0.5999, 0.4001], grad_fn=<SoftmaxBackward0>) y:  tensor([[0., 1.]])\n",
      "Train epoch:  0 i:  300 loss:  0.16996760666370392 out:  tensor([0.5877, 0.4123], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  400 loss:  0.3334299623966217 out:  tensor([0.5774, 0.4226], grad_fn=<SoftmaxBackward0>) y:  tensor([[0., 1.]])\n",
      "Train epoch:  0 i:  500 loss:  0.1873781979084015 out:  tensor([0.5671, 0.4329], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  600 loss:  0.20298603177070618 out:  tensor([0.5495, 0.4505], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  700 loss:  0.30635911226272583 out:  tensor([0.5535, 0.4465], grad_fn=<SoftmaxBackward0>) y:  tensor([[0., 1.]])\n",
      "Train epoch:  0 i:  800 loss:  0.2030264437198639 out:  tensor([0.5494, 0.4506], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  900 loss:  0.2183721661567688 out:  tensor([0.5327, 0.4673], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  1000 loss:  0.2229243665933609 out:  tensor([0.5279, 0.4721], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  1100 loss:  0.21536818146705627 out:  tensor([0.5359, 0.4641], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  1200 loss:  0.27963030338287354 out:  tensor([0.5288, 0.4712], grad_fn=<SoftmaxBackward0>) y:  tensor([[0., 1.]])\n",
      "Train epoch:  0 i:  1300 loss:  0.2751690447330475 out:  tensor([0.5246, 0.4754], grad_fn=<SoftmaxBackward0>) y:  tensor([[0., 1.]])\n",
      "Train epoch:  0 i:  1400 loss:  0.21891628205776215 out:  tensor([0.5321, 0.4679], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  1500 loss:  0.21827834844589233 out:  tensor([0.5328, 0.4672], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  1600 loss:  0.21792517602443695 out:  tensor([0.5332, 0.4668], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  1700 loss:  0.20616354048252106 out:  tensor([0.5459, 0.4541], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  1800 loss:  0.21712926030158997 out:  tensor([0.5340, 0.4660], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  1900 loss:  0.27537035942077637 out:  tensor([0.5248, 0.4752], grad_fn=<SoftmaxBackward0>) y:  tensor([[0., 1.]])\n",
      "Train epoch:  0 i:  2000 loss:  0.27531588077545166 out:  tensor([0.5247, 0.4753], grad_fn=<SoftmaxBackward0>) y:  tensor([[0., 1.]])\n",
      "Train epoch:  0 i:  2100 loss:  0.2282133400440216 out:  tensor([0.5223, 0.4777], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  2200 loss:  0.24138504266738892 out:  tensor([0.5087, 0.4913], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  2300 loss:  0.23256689310073853 out:  tensor([0.5177, 0.4823], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  2400 loss:  0.261963814496994 out:  tensor([0.5118, 0.4882], grad_fn=<SoftmaxBackward0>) y:  tensor([[0., 1.]])\n",
      "Train epoch:  0 i:  2500 loss:  0.24227742850780487 out:  tensor([0.5078, 0.4922], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  2600 loss:  0.2691271901130676 out:  tensor([0.5188, 0.4812], grad_fn=<SoftmaxBackward0>) y:  tensor([[0., 1.]])\n",
      "Train epoch:  0 i:  2700 loss:  0.27260008454322815 out:  tensor([0.5221, 0.4779], grad_fn=<SoftmaxBackward0>) y:  tensor([[0., 1.]])\n",
      "Train epoch:  0 i:  2800 loss:  0.2777036726474762 out:  tensor([0.5270, 0.4730], grad_fn=<SoftmaxBackward0>) y:  tensor([[0., 1.]])\n",
      "Train epoch:  0 i:  2900 loss:  0.223017618060112 out:  tensor([0.5278, 0.4722], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  3000 loss:  0.21457546949386597 out:  tensor([0.5368, 0.4632], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  3100 loss:  0.291852205991745 out:  tensor([0.5402, 0.4598], grad_fn=<SoftmaxBackward0>) y:  tensor([[0., 1.]])\n",
      "Train epoch:  0 i:  3200 loss:  0.2138078361749649 out:  tensor([0.5376, 0.4624], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  3300 loss:  0.22388581931591034 out:  tensor([0.5268, 0.4732], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  3400 loss:  0.2750134766101837 out:  tensor([0.5244, 0.4756], grad_fn=<SoftmaxBackward0>) y:  tensor([[0., 1.]])\n",
      "Train epoch:  0 i:  3500 loss:  0.22749240696430206 out:  tensor([0.5230, 0.4770], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  3600 loss:  0.2270526885986328 out:  tensor([0.5235, 0.4765], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  3700 loss:  0.28053519129753113 out:  tensor([0.5297, 0.4703], grad_fn=<SoftmaxBackward0>) y:  tensor([[0., 1.]])\n",
      "Train epoch:  0 i:  3800 loss:  0.22568997740745544 out:  tensor([0.5249, 0.4751], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  3900 loss:  0.22476118803024292 out:  tensor([0.5259, 0.4741], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  4000 loss:  0.2756398320198059 out:  tensor([0.5250, 0.4750], grad_fn=<SoftmaxBackward0>) y:  tensor([[0., 1.]])\n",
      "Train epoch:  0 i:  4100 loss:  0.27359795570373535 out:  tensor([0.5231, 0.4769], grad_fn=<SoftmaxBackward0>) y:  tensor([[0., 1.]])\n",
      "Train epoch:  0 i:  4200 loss:  0.2710823118686676 out:  tensor([0.5207, 0.4793], grad_fn=<SoftmaxBackward0>) y:  tensor([[0., 1.]])\n",
      "Train epoch:  0 i:  4300 loss:  0.23094068467617035 out:  tensor([0.5194, 0.4806], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  4400 loss:  0.26412591338157654 out:  tensor([0.5139, 0.4861], grad_fn=<SoftmaxBackward0>) y:  tensor([[0., 1.]])\n",
      "Train epoch:  0 i:  4500 loss:  0.27184075117111206 out:  tensor([0.5214, 0.4786], grad_fn=<SoftmaxBackward0>) y:  tensor([[0., 1.]])\n",
      "Train epoch:  0 i:  4600 loss:  0.2789326608181 out:  tensor([0.5281, 0.4719], grad_fn=<SoftmaxBackward0>) y:  tensor([[0., 1.]])\n",
      "Train epoch:  0 i:  4700 loss:  0.27309101819992065 out:  tensor([0.5226, 0.4774], grad_fn=<SoftmaxBackward0>) y:  tensor([[0., 1.]])\n",
      "Train epoch:  0 i:  4800 loss:  0.2329276204109192 out:  tensor([0.5174, 0.4826], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  4900 loss:  0.2377009391784668 out:  tensor([0.5125, 0.4875], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  5000 loss:  0.2371259480714798 out:  tensor([0.5130, 0.4870], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  5100 loss:  0.2776040732860565 out:  tensor([0.5269, 0.4731], grad_fn=<SoftmaxBackward0>) y:  tensor([[0., 1.]])\n",
      "Train epoch:  0 i:  5200 loss:  0.22866703569889069 out:  tensor([0.5218, 0.4782], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  5300 loss:  0.2374558299779892 out:  tensor([0.5127, 0.4873], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  5400 loss:  0.22950543463230133 out:  tensor([0.5209, 0.4791], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  5500 loss:  0.2662820816040039 out:  tensor([0.5160, 0.4840], grad_fn=<SoftmaxBackward0>) y:  tensor([[0., 1.]])\n",
      "Train epoch:  0 i:  5600 loss:  0.26644662022590637 out:  tensor([0.5162, 0.4838], grad_fn=<SoftmaxBackward0>) y:  tensor([[0., 1.]])\n",
      "Train epoch:  0 i:  5700 loss:  0.23995402455329895 out:  tensor([0.5101, 0.4899], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  5800 loss:  0.23626242578029633 out:  tensor([0.5139, 0.4861], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  5900 loss:  0.2780609130859375 out:  tensor([0.5273, 0.4727], grad_fn=<SoftmaxBackward0>) y:  tensor([[0., 1.]])\n",
      "Train epoch:  0 i:  6000 loss:  0.21747343242168427 out:  tensor([0.5337, 0.4663], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n",
      "Train epoch:  0 i:  6100 loss:  0.2246604561805725 out:  tensor([0.5260, 0.4740], grad_fn=<SoftmaxBackward0>) y:  tensor([[1., 0.]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [38], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     32\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 33\u001b[0m out \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m     34\u001b[0m loss \u001b[39m=\u001b[39m loss_function(out, y)\n\u001b[0;32m     35\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [31], line 31\u001b[0m, in \u001b[0;36mcrypto_classfier_ver1.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mT\n\u001b[0;32m     30\u001b[0m x1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet(x[\u001b[39m0\u001b[39m])\n\u001b[1;32m---> 31\u001b[0m x2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet(x[\u001b[39m1\u001b[39;49m])\n\u001b[0;32m     32\u001b[0m x3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet(x[\u001b[39m2\u001b[39m])\n\u001b[0;32m     33\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((x1,x2,x3),\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\activation.py:102\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 102\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu_(\u001b[39minput\u001b[39m)\n\u001b[0;32m   1456\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m   1458\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#setting loss function and optimizer\n",
    "loss_functions = [nn.MSELoss(), nn.BCELoss(),nn.CrossEntropyLoss()]\n",
    "optimizers = [torch.optim.Adam, torch.optim.SGD]\n",
    "models = [crypto_classfier_ver1(), crypto_classfier_ver2(), crypto_classfier_ver3(), crypto_classfier_ver4()]\n",
    "lr = 0.0001\n",
    "epochs = 30\n",
    "batch_size = 1 #batch size 是一次讀取多少筆資料 我這邊只能設1 因為我們的資料是一筆一筆的\n",
    "#training\n",
    "for model in models:\n",
    "    for loss_function in loss_functions:\n",
    "        for optimizer in optimizers:\n",
    "            print(\"model: \", model.name)\n",
    "            print(\"loss_function: \", loss_function)\n",
    "            print(\"optimizer: \", optimizer)\n",
    "            print(\"lr: \", lr)\n",
    "            print(\"epochs: \", epochs)\n",
    "            print(\"batch_size: \", batch_size)\n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "            model.to(device)\n",
    "            print(\"device: \", device)\n",
    "            optimizer = optimizer(model.parameters(), lr=lr)\n",
    "            train_loss = []\n",
    "            test_loss = []\n",
    "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            for epoch in range(epochs):\n",
    "                for i, (x, y) in enumerate(train_loader):\n",
    "                    x = x[0]\n",
    "                    y = y\n",
    "                    x = x.to(device)\n",
    "                    y = y.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    out = model(x)\n",
    "                    loss = loss_function(out, y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    train_loss.append(loss.item())\n",
    "                    if i % 1000 == 0:\n",
    "                        print(\"Train epoch: \", epoch, \"i: \", i, \"loss: \", loss.item(), \"out: \", out, \"y: \", y)\n",
    "                for i, (x, y) in enumerate(test_loader):\n",
    "                    x = x[0]\n",
    "                    y = y\n",
    "                    x = x.to(device)\n",
    "                    y = y.to(device)\n",
    "                    out = model(x)\n",
    "                    loss = loss_function(out, y)\n",
    "                    test_loss.append([loss.item(), model.parameters()])\n",
    "                    if i % 1000 == 0:\n",
    "                        print(\"epoch: \", epoch, \"i: \", i, \"loss: \", loss.item())\n",
    "                if epoch % 10 == 0:\n",
    "                    print(\"epoch: \", epoch, \"train_loss: \", train_loss[-1], \"test_loss: \", test_loss[-1])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
